[DEFAULT]
# Backbone architecture of the model - options are [distilbert, roberta]
BaseModel = roberta
BatchSize = 64
MaxEpochs = 3
LR = 2e-5
DataDir = twitter-datasets/data
SubmissionDir = submissions
DatasetPos = final_train_pos_full.txt
DatasetNeg = final_train_neg_full.txt
TestData = final_test_data.txt
CheckpointDir = checkpoints

#Load a model from a checkpoint
LoadCheckpoint = False
CheckpointName = RoBERTa_stats_final.pth

#Train only the classifier head
OnlyClassifier = False

#Using 10% of the training steps as warmup
WarmupSteps = True

[Basic]
#Specific huggingface model from which to load the architecture - options are [distilbert-base-uncased-finetuned-sst-2-english, cardiffnlp/twitter-roberta-base-sentiment-latest]
ModelType = cardiffnlp/twitter-roberta-base-sentiment-latest

[BiLSTM]
#Use convolutional layers in the classifier head
UseCNN = False

[Stats]
#Use lexical features to augment the input
UseVader = True

#Use LDA topic modelling features to augment the input
UseLDA = True

#Use statistical features to augment the input
UseStats = True
StatsPos = pos_stats.csv
StatsNeg = neg_stats.csv
StatsTest = test_stats.csv