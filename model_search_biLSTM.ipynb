{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5446c090",
   "metadata": {},
   "source": [
    "# Parameter Search Notebook for BiLSTM Based Models\n",
    "\n",
    "-----------------------------------------------------\n",
    "\n",
    "This notebook contains different stages of training hyperparameter search, model structure search, sample comparison of embedding types and model extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d099c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.BiLSTM.embed_utils import *\n",
    "from models.utils import *\n",
    "from models.BiLSTM.biLSTM_model import *\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import wandb\n",
    "from keras import backend as K\n",
    "\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afd2ffd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n",
      "Found device at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != \"/device:GPU:0\":\n",
    "  device_name = \"/cpu:0\"\n",
    "print('Found device at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5f9f6e",
   "metadata": {},
   "source": [
    "### Path variables & loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "146b623c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prefix  = \"emoticons_and_haha_all_simple_methods_and_dictionary_s2s_\"\n",
    "data_folder  = 'twitter-datasets/data/'\n",
    "glove_folder = 'twitter-datasets/glove_twitter'\n",
    "ckpt_folder  = \"checkpoints\"\n",
    "\n",
    "logging_file = os.path.join(ckpt_folder, \"param_search_logs\", \"param_search_logs.txt\") \n",
    "\n",
    "\n",
    "data_path     = data_folder + data_prefix + \"train_\"\n",
    "path_pos = data_path + \"pos_full.txt\"\n",
    "path_neg = data_path + \"neg_full.txt\"\n",
    "\n",
    "base_save_suffix = \"param_search_\"\n",
    "\n",
    "tweets, labels = load_tweets(path_pos, path_neg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea89a69",
   "metadata": {},
   "source": [
    "### Model parameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18f3872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_type_list = [\"glove\",\"ftxt\"]\n",
    "\n",
    "epochs          = 10\n",
    "batch_size      = 500\n",
    "lr_list         = [0.001,0.0005,0.0001]\n",
    "dropout_list    = [0, 0.5]\n",
    "\n",
    "cell_size_list  = [50,100]\n",
    "\n",
    "num_conv1D_list = [0,1,2]\n",
    "conv_dim_list   = [16, 32]\n",
    "\n",
    "num_dense_list  = [1,2]\n",
    "dense_dim       = 16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153b9247",
   "metadata": {},
   "source": [
    "*********************************\n",
    "\n",
    "## 1. Search Over Base Model Structure: BiLSTM + Dense Layers\n",
    "\n",
    "Used to assess the effect of: learning rate, the size of the cell of each LSTM in the BiLSTM, number of dense layers\n",
    "\n",
    "-----------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7dc98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_dim=16\n",
    "embed_dim=100\n",
    "embed_type = \"ftxt\"\n",
    "line_str = \"----------------------------------------\\n\"\n",
    "star_str = \"****************************************\\n\"\n",
    "\n",
    "best_config = \"\"\n",
    "best_acc = 0\n",
    "\n",
    "for cell_size in cell_size_list:\n",
    "    for lr in lr_list:\n",
    "        for num_dense in num_dense_list:\n",
    "\n",
    "                with tf.device(device_name):\n",
    "\n",
    "                    model_config = f'embed type: ftxt | cell_size: {cell_size} | num_dense: {num_dense} | dense_dim: 16 | lr: {lr} | optim: {optimizer}\\n'\n",
    "                    print(star_str)\n",
    "                    print(model_config)\n",
    "                    print(line_str)\n",
    "                    \n",
    "                    df=open(logging_file,'a')\n",
    "                    df.write(star_str )\n",
    "                    df.write(model_config )\n",
    "                    df.write(line_str )  \n",
    "                    df.close()\n",
    "                    \n",
    "                    #save_suffix = base_save_suffix + \"_ftxt\" + f\"_cell_{cell_size}\" + f\"_num_dense_{num_dense}\"+ f\"_lr_{lr}\" + f\"_optim_{optimizer}\"\n",
    "\n",
    "\n",
    "                    model = biLSTM_model(save_suffix =base_save_suffix, \n",
    "                                           #embedding params\n",
    "                                           embed_dim = embed_dim,\n",
    "                                           embed_type= embed_type, \n",
    "                                           max_len   = max_len,  \n",
    "\n",
    "                                           #model extension options\n",
    "                                           augment_lda    = False, \n",
    "                                           augment_stats  = False, \n",
    "                                           augment_vader  = False,\n",
    "\n",
    "                                           #path params\n",
    "                                           ckpt_folder = ckpt_folder,\n",
    "                                           embed_path = glove_folder, \n",
    "                                          )\n",
    "\n",
    "\n",
    "                    accuracy, f1, precision, recall = model.train_model( tweets, labels, \n",
    "\n",
    "                            # training params\n",
    "                            lr         = lr, \n",
    "                            epochs     = epochs, \n",
    "                            optim      = optimizer,                                         \n",
    "\n",
    "                            #LSTM\n",
    "                            cell_size  = cell_size,\n",
    "                            #Conv\n",
    "                            num_conv1D = 0, \n",
    "                            #Dense\n",
    "                            dense_dim  = dense_dim, num_dense = num_dense,  )\n",
    "                    \n",
    "                    model_result = f\"accuracy: {accuracy}, f1: {f1}, precision: {precision}, recall: {recall}\"\n",
    "                    \n",
    "                    df=open(logging_file,'a')\n",
    "                    df.write(model_result )\n",
    "                    df.close()\n",
    "                    \n",
    "                    del model\n",
    "                    K.clear_session()\n",
    "                    \n",
    "                    if accuracy > best_acc: best_config = model_config\n",
    "                        \n",
    "                        \n",
    "print(\"Best Config: \", best_config)\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eb36d2",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------\n",
    "\n",
    "## 2. Search Over 1D Conv Models:  BiLSTM + 1D Conv Layers + Dense Layers\n",
    "\n",
    "* Used to assess the effect of different numbers of one dimensional layers on top of sequence output from BiLSTM\n",
    "* Used to assess number of convolutional filters per layer \n",
    "* At each stage, the two embedding models are also compared -- commented out when not needed\n",
    "\n",
    "-----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0322bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_str = \"----------------------------------------\\n\"\n",
    "star_str = \"****************************************\\n\"\n",
    "\n",
    "best_config = \"\"\n",
    "best_acc = 0\n",
    "            \n",
    "for num_conv1D in num_conv1D_list:\n",
    "    for conv_dim in conv_dim_list:\n",
    "        for embed_type in embed_type_list:\n",
    "\n",
    "                with tf.device(device_name):\n",
    "\n",
    "                    model_config = f'| embed_type: {embed_type} | num_conv1D: {num_conv1D} | conv_dim: 16 \\n'\n",
    "                    print(star_str)\n",
    "                    print(model_config)\n",
    "                    print(line_str)\n",
    "                    \n",
    "                    df=open(logging_file,'a')\n",
    "                    df.write(star_str )\n",
    "                    df.write(model_config )\n",
    "                    df.write(line_str )  \n",
    "                    df.close()\n",
    "                    \n",
    "                    #save_suffix = base_save_suffix + \"_ftxt\" + f\"_cell_{cell_size}\" + f\"_num_dense_{num_dense}\"+ f\"_lr_{lr}\" + f\"_optim_{optimizer}\"\n",
    "\n",
    "\n",
    "                    model = biLSTM_model(save_suffix =base_save_suffix, \n",
    "                                         \n",
    "                                           embed_type = embed_type,\n",
    "\n",
    "                                           #model extension options\n",
    "                                           augment_lda    = False, \n",
    "                                           augment_stats  = False, \n",
    "                                           augment_vader  = False, \n",
    "\n",
    "                                           #path params\n",
    "                                           ckpt_folder = ckpt_folder,\n",
    "                                           embed_path = glove_folder \n",
    "                                          )\n",
    "\n",
    "\n",
    "                    accuracy, f1, precision, recall = model.train_model( tweets, labels,                                   \n",
    "                            num_conv1D = num_conv1D, \n",
    "                            conv_dim = conv_dim  )\n",
    "                    \n",
    "                    model_result = f\"accuracy: {accuracy}, f1: {f1}, precision: {precision}, recall: {recall}\\n\"\n",
    "                    \n",
    "                    df=open(logging_file,'a')\n",
    "                    df.write(model_result )\n",
    "                    df.close()\n",
    "                    \n",
    "                    del model\n",
    "                    K.clear_session()\n",
    "                    \n",
    "                    if accuracy > best_acc: best_config = model_config\n",
    "                        \n",
    "                        \n",
    "print(\"Best Config: \", best_config)\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa81e8ec",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------\n",
    "\n",
    "## 3. Effects of Model Extensions:  BiLSTM + 1D Conv Layers + Dense Layers + Extensions\n",
    "\n",
    "### 3.1. Model Statistics\n",
    "### 3.2. Lexical Features added to embeddings\n",
    "### 3.3. Adding the averaged sentence embedding as auxiliary input\n",
    "### 3.4. Latent Dirichlet Allocation Topic Modeling Vector as auxiliary input\n",
    "-----------------------------------------------------------------------------------\n",
    "\n",
    "## 3.1 Lexical Features \n",
    "\n",
    "Lexical features are word polarity scores obtained using VADER sentiment lexicon. They are concatenated to the embedding vector of each word within the embedding matrix used for the model. Refer to models/BiLSTM/embed_utils.py to inspect implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5d5bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_str = \"----------------------------------------\\n\"\n",
    "star_str = \"****************************************\\n\"\n",
    "\n",
    "for embed_type in embed_type_list:\n",
    "    with tf.device(device_name):\n",
    "\n",
    "                    model_config = f'embed: {embed_type} | standard params + lexical features + no 1conv + 2 dense\\n'\n",
    "                    print(star_str)\n",
    "                    print(model_config)\n",
    "                    print(line_str)\n",
    "                    \n",
    "                    df=open(logging_file,'a')\n",
    "                    df.write(star_str )\n",
    "                    df.write(model_config )\n",
    "                    df.write(line_str )  \n",
    "                    df.close()\n",
    "                    \n",
    "                    #save_suffix = base_save_suffix + \"_ftxt\" + f\"_cell_{cell_size}\" + f\"_num_dense_{num_dense}\"+ f\"_lr_{lr}\" + f\"_optim_{optimizer}\"\n",
    "\n",
    "\n",
    "                    model = biLSTM_model(save_suffix =base_save_suffix, \n",
    "                                         \n",
    "                                           embed_type = embed_type,\n",
    "\n",
    "                                           #model extension options\n",
    "                                           augment_stats   = False, \n",
    "                                           augment_vader   = True, \n",
    "                                           augment_lda     = False,\n",
    "\n",
    "                                           #path params\n",
    "                                           ckpt_folder = ckpt_folder,\n",
    "                                           embed_path = glove_folder, \n",
    "                                          )\n",
    "\n",
    "\n",
    "                    accuracy, f1, precision, recall = model.train_model( tweets, labels, num_conv1D = 0)\n",
    "                    \n",
    "                    model_result = f\"accuracy: {accuracy}, f1: {f1}, precision: {precision}, recall: {recall}\"\n",
    "                    \n",
    "                    df=open(logging_file,'a')\n",
    "                    df.write(model_result )\n",
    "                    df.close()\n",
    "                    \n",
    "                    del model\n",
    "                    K.clear_session()\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830c0b38",
   "metadata": {},
   "source": [
    "## 3.2 Lexical Features + LDA\n",
    "\n",
    "The training dataset is used to train a Latent Dirichlet Allocation topic model. The hypothesis is that if there are underlying themes / topics present in the tweet data and if certain themes are more likely to be positive / negative, topic distribution information can help the overall model leverage this information. LDA Model is trained with 10 topics, 10 dimensional vectors for each input tweet are igven as an auxiliary input between the final two dense layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228675c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_str = \"----------------------------------------\\n\"\n",
    "star_str = \"****************************************\\n\"\n",
    "\n",
    "for embed_type in embed_type_list:\n",
    "    with tf.device(device_name):\n",
    "\n",
    "                    model_config = f'embed: {embed_type} | standard params + lda + lexical features  + no 1conv + 2 dense\\n'\n",
    "                    print(star_str)\n",
    "                    print(model_config)\n",
    "                    print(line_str)\n",
    "                    \n",
    "                    df=open(logging_file,'a')\n",
    "                    df.write(star_str )\n",
    "                    df.write(model_config )\n",
    "                    df.write(line_str )  \n",
    "                    df.close()\n",
    "                    \n",
    "                    #save_suffix = base_save_suffix + \"_ftxt\" + f\"_cell_{cell_size}\" + f\"_num_dense_{num_dense}\"+ f\"_lr_{lr}\" + f\"_optim_{optimizer}\"\n",
    "\n",
    "\n",
    "                    model = biLSTM_model(save_suffix =base_save_suffix, \n",
    "                                         \n",
    "                                           embed_type = embed_type,\n",
    "\n",
    "                                           #model extension options\n",
    "                                           augment_stats   = False, \n",
    "                                           augment_vader   = True, \n",
    "                                           augment_lda   = True,\n",
    "\n",
    "                                           #path params\n",
    "                                           ckpt_folder = ckpt_folder,\n",
    "                                           embed_path = glove_folder, \n",
    "                                          )\n",
    "\n",
    "\n",
    "                    accuracy, f1, precision, recall = model.train_model( tweets, labels, num_conv1D = 0)\n",
    "                    \n",
    "                    model_result = f\"accuracy: {accuracy}, f1: {f1}, precision: {precision}, recall: {recall}\"\n",
    "                    \n",
    "                    df=open(logging_file,'a')\n",
    "                    df.write(model_result )\n",
    "                    df.close()\n",
    "                    \n",
    "                    del model\n",
    "                    K.clear_session()\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a157652",
   "metadata": {},
   "source": [
    "## 3.4 Model Statistics + Lexical Features + LDA Topic Features\n",
    "\n",
    "Certain types of tokens carry patterns that can be manually manipulated for preprocessing or feature extraction. Abbreviations/contractions, emoticons, variations of \"haha\" type laughing expressions are some of these identifiers which may express information an embedding model may miss due to model vocabulary limits or not enough training data. Thus, they are treated to obtain uniformity in various preprocessing steps.\n",
    "\n",
    "However, another way of using these identifiers is not to use them in peprocessing but create an explicity feature vector out of their frequencies in a tweet. This is what is done in this model extension. Just like LDA topics, token statistics vectors are concatenated as an auxiliary input between the last two dense layers of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61303af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_str = \"----------------------------------------\\n\"\n",
    "star_str = \"****************************************\\n\"\n",
    "\n",
    "data_folder  = 'twitter-datasets/data/'\n",
    "glove_folder = 'twitter-datasets/glove_twitter'\n",
    "ckpt_folder  = \"checkpoints\"\n",
    "\n",
    "logging_file = os.path.join(ckpt_folder, \"param_search_logs\", \"param_search_logs.txt\") \n",
    "\n",
    "\n",
    "data_path     = data_folder + data_prefix + \"train_\"\n",
    "path_pos = data_path + \"pos_full.txt\"\n",
    "path_neg = data_path + \"neg_full.txt\"\n",
    "\n",
    "base_save_suffix = \"param_search_\"\n",
    "\n",
    "tweets, labels = load_tweets(path_pos, path_neg)\n",
    "\n",
    "for embed_type in embed_type_list:\n",
    "    with tf.device(device_name):\n",
    "\n",
    "                    model_config = f'embed: {embed_type} | standard params + model statistics + lexical features + avg sent embedss  + 2 conv + 2 dense\\n'\n",
    "                    print(star_str)\n",
    "                    print(model_config)\n",
    "                    print(line_str)\n",
    "                    \n",
    "                    df=open(logging_file,'a')\n",
    "                    df.write(star_str )\n",
    "                    df.write(model_config )\n",
    "                    df.write(line_str )  \n",
    "                    df.close()\n",
    "                    \n",
    "              \n",
    "\n",
    "                    model = biLSTM_model(save_suffix =base_save_suffix, \n",
    "                                         \n",
    "                                           embed_type = embed_type,\n",
    "\n",
    "                                           #model extension options\n",
    "                                           augment_stats   = True, \n",
    "                                           augment_vader   = True, \n",
    "                                           augment_lda     = True,\n",
    "\n",
    "                                           #path params\n",
    "                                           ckpt_folder = ckpt_folder,\n",
    "                                           embed_path = glove_folder, \n",
    "                                          )\n",
    "                    \n",
    "                    \n",
    "\n",
    "\n",
    "                    accuracy, f1, precision, recall = model.train_model( tweets, labels, num_conv1D = 0)\n",
    "                    \n",
    "                    model_result = f\"accuracy: {accuracy}, f1: {f1}, precision: {precision}, recall: {recall}\"\n",
    "                    \n",
    "                    df=open(logging_file,'a')\n",
    "                    df.write(model_result )\n",
    "                    df.close()\n",
    "                    \n",
    "                    del model\n",
    "                    K.clear_session()\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5597d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75b11b1f",
   "metadata": {},
   "source": [
    "# --------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "528247e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "from models.BiLSTM.embed_utils import *\n",
    "from models.utils import *\n",
    "from models.lda import LDA_Model\n",
    "\n",
    "import os, io, json \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Activation, Dropout, Dense, Input, Bidirectional,Conv1D,MaxPool1D,Flatten,Embedding, Lambda, Concatenate\n",
    "from tensorflow.keras import models, backend\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence, tokenizer_from_json\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "class attention(tf.keras.layers.Layer):\n",
    "    def __init__(self,return_sequences=False):\n",
    "        #super(attention,self).__init__(**kwargs)\n",
    "        self.return_sequences = return_sequences\n",
    "\n",
    "        super(attention,self).__init__()\n",
    " \n",
    "    def build(self,input_shape):\n",
    "        self.W=self.add_weight(name='attention_weight', shape=(input_shape[-1],1), \n",
    "                               initializer='random_normal', trainable=True)\n",
    "        self.b=self.add_weight(name='attention_bias', shape=(input_shape[1],1), \n",
    "                               initializer='zeros', trainable=True)        \n",
    "        super(attention, self).build(input_shape)\n",
    " \n",
    "    def call(self,x):\n",
    "\n",
    "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
    "        e = K.squeeze(e, axis=-1)   \n",
    "        alpha = K.softmax(e)\n",
    "        alpha = K.expand_dims(alpha, axis=-1)\n",
    "        \n",
    "        #final context vector\n",
    "        context = x * alpha\n",
    "        \n",
    "        if self.return_sequences:\n",
    "            return context\n",
    "        \n",
    "        context = K.sum(context, axis=1)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d0b77d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class biLSTM_model1:\n",
    "\n",
    "\n",
    "    \n",
    "    def __init__(self, save_suffix, embed_type=\"ftxt\", embed_dim=100,\n",
    "                    augment_vader=False, augment_lda=False, augment_stats=False,\n",
    "                    max_len=150,  \n",
    "                    data_path = 'twitter-datasets/data',\n",
    "                    embed_path = 'twitter-datasets/glove_twitter',\n",
    "                    ckpt_folder = \"checkpoints\"):\n",
    "\n",
    "        \"\"\"Class constructor\n",
    "        \n",
    "        Args:\n",
    "\n",
    "            save_suffix   (str) : An identifier used do distinguish save files for a specific run of the bilstm model, tokenizer, lda model\n",
    "            embed_type    (str) : choose an embedding model type between the following options: \"ftxt\" , \"glove\"\n",
    "            embed_dim     (int) : The dimension of the word embeddings produced by the embedding model\n",
    "            augment_vader (bool): Whether to extend each word embedding by its sentiment score according to the sentiment lexicon \"vader\"\n",
    "            augment_lda   (bool): Whether to apply Latent Dirichlet Allocation topic modeling to the data, \n",
    "                                  produce topic distributions, and use them as an auxiliary input vector in the model\n",
    "            augment_stats (bool): Whether to add a vector of sentence statistics about the frequency of certain identifiers as an auxiliary input \n",
    "            max_len       (int) : The maximum number of tokens allowed for an input tweet, all tweets truncated or padded up to this value\n",
    "            data_path     (str) : The folder path where training and test datasets are stored\n",
    "            embed_path    (str) : The folder path where pretrained glove embeddings are stored and new fasttext embeddings are saved\n",
    "            ckpt_folder   (str) : The folder path including the subfolders for different models' save files / checkpoints\n",
    "\n",
    "        \"\"\"\n",
    "    \n",
    "        self.embed_path = embed_path\n",
    "        self.ckpt_folder = ckpt_folder\n",
    "        self.save_suffix = save_suffix\n",
    "        self.data_path   = data_path\n",
    "        \n",
    "        tokenizer_name = f\"tokenizer_{self.save_suffix}.json\"\n",
    "        self.tokenizer_path = os.path.join(self.ckpt_folder, \"tokenizer\", tokenizer_name)\n",
    "        self.lda_folder = os.path.join(self.ckpt_folder, \"lda\")\n",
    "        \n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.ftxt_model = None\n",
    "        self.glove_model = None\n",
    "        self.lda_model = None\n",
    " \n",
    "        \n",
    "        self.model_path = \"\"\n",
    "        self.embed_dim        = embed_dim\n",
    "        self.embed_type       = embed_type\n",
    "\n",
    "        self.augment_stats    = augment_stats\n",
    "        self.augment_vader    = augment_vader\n",
    "        self.augment_lda      = augment_lda\n",
    "        self.max_len          = max_len\n",
    "        \n",
    "        # checkpoint name construction\n",
    "                    \n",
    "        if embed_type == \"glove\":\n",
    "            self.model_path = os.path.join(self.ckpt_folder, \"biLSTM\", f\"biLSTM_glove_d{self.embed_dim}\")           \n",
    "        elif embed_type == \"ftxt\":\n",
    "            self.model_path = os.path.join(self.ckpt_folder, \"biLSTM\", f\"biLSTM_ftxt_d{self.embed_dim}\")  \n",
    "        #else:\n",
    "        #    self.model_path = f\"{ckpt_folder}/biLSTM_concat_d{embed_dim}x2\"\n",
    "        \n",
    "        if self.augment_vader:\n",
    "\n",
    "            self.model_path += \"_vader\"\n",
    "\n",
    "        if self.augment_lda:\n",
    "\n",
    "            self.model_path += \"_lda\"\n",
    "            \n",
    "        if self.augment_stats:\n",
    "         \n",
    "            self.model_path += \"_stats\"\n",
    "        \n",
    "        self.model_path += f\"_{self.save_suffix}\"\n",
    "    \n",
    "    \n",
    "    def build_model(self, embed_matrix, num_LSTM = 1, num_conv1D = 0, conv_dim=16,\n",
    "                       num_dense= 2, dense_dim=16, cell_size = 100, \n",
    "                       dropout= 0.5):\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "        X_in = Input((self.max_len,))\n",
    "\n",
    "        input_dim, embed_vector_len = embed_matrix.shape\n",
    "\n",
    "        embedding_layer = Embedding(input_dim=input_dim, \n",
    "                                output_dim=embed_vector_len, \n",
    "                                input_length=self.max_len, \n",
    "                                mask_zero=   True,\n",
    "                                weights = [embed_matrix], \n",
    "                                trainable=False)\n",
    "    \n",
    "        embeddings = embedding_layer(X_in)\n",
    "\n",
    "        if num_LSTM:\n",
    "            \n",
    "            X = Bidirectional(LSTM(cell_size, activation='tanh', return_sequences=True))(embeddings)\n",
    "            X = Dropout(dropout)(X)\n",
    "\n",
    "            #if there are more than 2 lstm cells\n",
    "\n",
    "            for i in range(num_LSTM-1):\n",
    "                \n",
    "                if not num_conv1D and i == num_LSTM-2:\n",
    "                    X = Bidirectional(LSTM(cell_size//2, activation='tanh', return_sequences=False))(X)\n",
    "                else:\n",
    "                    X = Bidirectional(LSTM(cell_size//2, activation='tanh', return_sequences=True))(X)\n",
    "                X = Dropout(dropout)(X)\n",
    "    \n",
    "        else:\n",
    "            X = embeddings\n",
    "            \n",
    "        X = attention(return_sequences = bool(num_conv1D))(X)\n",
    "            \n",
    "        # if desired, adding Conv1D + dropout + maxpool layers\n",
    "        if num_conv1D:\n",
    "\n",
    "            for _ in range(num_conv1D):\n",
    "\n",
    "                X = Conv1D(filters=conv_dim, kernel_size=3, strides=1, padding=\"valid\", activation='relu')(X)\n",
    "                X = Dropout(dropout)(X)\n",
    "                X = MaxPool1D(pool_size=2, strides=2, padding=\"valid\")(X)\n",
    "    \n",
    "            X = Flatten()(X)\n",
    "\n",
    "\n",
    "        # adding final fully connected layers\n",
    "        for _ in range(num_dense-1):\n",
    "            X = Dense(dense_dim, activation='relu')(X)\n",
    "\n",
    "        #Adding the sentence length information before final FC layer\n",
    "        input_list = [X_in]\n",
    "\n",
    "        if self.augment_stats:\n",
    "            X_stats = Input((10,))\n",
    "            X = Concatenate()([X, X_stats])\n",
    "            input_list.append(X_stats)\n",
    "\n",
    "        if self.augment_lda:\n",
    "            X_lda = Input((10,))\n",
    "            X = Concatenate()([X, X_lda])\n",
    "            input_list.append(X_lda)\n",
    "       \n",
    "\n",
    "\n",
    "        X = Dense(1, activation='sigmoid')(X)\n",
    "\n",
    "        self.model = models.Model(inputs=input_list, outputs=X)\n",
    "\n",
    "    \n",
    "                   \n",
    "              \n",
    "    \n",
    "    def train_model(self, X, Y,\n",
    "                    num_LSTM = 1, num_conv1D = 0, conv_dim=32,\n",
    "                    num_dense= 2, dense_dim=16, cell_size = 100, dropout= 0.5, \n",
    "                    # training params\n",
    "                    batch_size = 500, lr= 0.0005, epochs=20, optim=\"adam\", \n",
    "                    random_state=0 ):\n",
    "\n",
    "        \"\"\"Model training function\n",
    "        \n",
    "        Args:\n",
    "            -- Inputs\n",
    "            X (np.ndarray) : input tweets, array of strings\n",
    "            Y (np.ndarray) : input labels, array of integers\n",
    "\n",
    "            -- Architecture Parameters\n",
    "            num_LSTM   (int) : Number of Bidirectional LSTM layers\n",
    "            num_conv1D (int) : Number of 1 dimensional convolution layers to apply sequentially to sequential input\n",
    "            conv_dim   (int) : Number of convolutional filters at each 1D Conv layer\n",
    "            num_dense  (int) : Number of fully connected layers \n",
    "            dense_dim  (int) : Size of fully connected layers\n",
    "            cell_size  (int) : Size of the internal cell of the LSTMs in the Bidirectional LSTM\n",
    "            dropout    (float) : Dropout probability\n",
    "\n",
    "            -- Training Parameters\n",
    "            batch_size   (int) : size of training batches \n",
    "            lr           (float) : learning rate\n",
    "            epochs       (int) : number of training epochs\n",
    "            optim        (str) : type of optimizer for training - options: \"adam\", \"sgd\", \"rmsprop\"\n",
    "            random_state (int) : the seed used to condition the random numbers of tensorflow and numpy for reproducible results\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "        # setting random seeds for reproducable results\n",
    "        np.random.seed(random_state)\n",
    "        tf.random.set_seed(random_state)\n",
    "\n",
    "        #preparing model training inputs\n",
    "        #X_train, X_dev -> for embedding generation\n",
    "        train_inputs, dev_inputs, X_train, Y_train, Y_dev = self._prepare_inputs(X,Y,  random_state = random_state )\n",
    "        \n",
    "        word_idx = self.tokenizer.word_index\n",
    "       \n",
    "        \n",
    "        # choosing embedding type and forming a lookup matrix for token embeddings\n",
    "        if self.embed_type == \"glove\":\n",
    "            glove_path = f\"{self.embed_path}/glove.twitter.27B.{self.embed_dim}d.txt\"\n",
    "            word_to_vec_map = read_glove_vector(glove_path)\n",
    "            embed_matrix = make_embed_matrix( word_idx, word_to_vec_map, augment_vader=self.augment_vader)\n",
    "            \n",
    "        elif self.embed_type == \"ftxt\":\n",
    "\n",
    "            word_to_vec_map = train_ftxt(X_train, embed_dim=self.embed_dim, save_suffix=self.save_suffix, ckpt_folder=self.embed_path)\n",
    "            embed_matrix = make_embed_matrix( word_idx, word_to_vec_map, augment_vader=self.augment_vader)\n",
    "            \n",
    "        # building model architecture\n",
    "        self.build_model(embed_matrix=embed_matrix, \n",
    "        cell_size = cell_size, num_conv1D=num_conv1D, conv_dim=conv_dim, dropout= dropout, num_LSTM=num_LSTM, dense_dim=dense_dim, num_dense=num_dense)\n",
    "                \n",
    "        # setting callback functions to automatically lower learning rate and then stop when at risk of overfitting\n",
    "        early = EarlyStopping(monitor=\"val_loss\", patience=5, verbose=1)\n",
    "        redonplat = ReduceLROnPlateau(monitor=\"val_loss\", patience=2, verbose=1, factor=0.5)\n",
    "        checkpoint = ModelCheckpoint( filepath=self.model_path, save_weights_only=False, \n",
    "                                  monitor='val_accuracy', mode='max', save_best_only=True)\n",
    "\n",
    "        callbacks_list = [early, redonplat,checkpoint] \n",
    "        \n",
    "        #choosing optimizer \n",
    "        if  optim==\"adam\":\n",
    "            optim = Adam(learning_rate = lr)\n",
    "        elif  optim==\"sgd\":\n",
    "            optim = SGD(learning_rate = lr)\n",
    "        else:\n",
    "            optim = RMSprop(learning_rate = lr)\n",
    "            \n",
    "\n",
    "        self.model.compile(optimizer=optim, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        self.model.summary()\n",
    "    \n",
    "        self.model.fit(x=train_inputs, y=Y_train, batch_size=batch_size, epochs=epochs,shuffle=True,\n",
    "              validation_data=(dev_inputs,Y_dev), callbacks=callbacks_list )\n",
    "        \n",
    "        # evaluating on dev set\n",
    "        eval_results = self.evaluate(dev_inputs, Y_dev, prepare_input=False)\n",
    "\n",
    "        self.save_model()\n",
    "\n",
    "        return eval_results\n",
    "\n",
    "\n",
    "              \n",
    "                    \n",
    "    def predict(self, X, prepare_input=True):  \n",
    "\n",
    "\n",
    "        \"\"\"Model prediction function \n",
    "        \n",
    "        Args:\n",
    "            \n",
    "            X (np.ndarray) OR list of np.ndarray : either prepared list of all model inputs or unprepared array of tweet strings\n",
    "            prepare_input (bool) : whether to prepare the input by tokenizing, extracting statistics vectors, extracting topic distributions if required\n",
    "\n",
    "        Returns:\n",
    "            Y_pred (np.ndarray) : model predictions\n",
    "\n",
    "        \"\"\"\n",
    "                   \n",
    "        if prepare_input: \n",
    "            X_tokenized = self._tokenize(X)\n",
    "            X_input = [X_tokenized]\n",
    "\n",
    "            if self.augment_stats:\n",
    "                X_stats = np.array( sentence_statistics(X) )\n",
    "                X_input.append(X_stats)\n",
    "\n",
    "            if self.augment_lda:\n",
    "                X_topics   = self.lda_model.process_new_data(X)\n",
    "                X_input.append(X_topics)\n",
    "\n",
    "        else:\n",
    "            X_input = X\n",
    "\n",
    "        Y_pred = self.model.predict(x=X_input)\n",
    "        Y_pred = (Y_pred[:,0] > 0.5).astype(np.int)\n",
    "        return Y_pred\n",
    "                    \n",
    "    \n",
    "    def evaluate(self, X, Y, prepare_input=True):\n",
    "\n",
    "        \"\"\"Generates evaluation metrics for given data and ground truth labels\n",
    "        \n",
    "        Args:\n",
    "            \n",
    "            X (np.ndarray) OR list of np.ndarray : either prepared list of all model inputs or unprepared  array of tweet strings\n",
    "            Y (np.ndarray)  : ground truth labels \n",
    "            prepare_input (bool) : whether to prepare the input by tokenizing, extracting statistics vectors, extracting topic distributions if required\n",
    "\n",
    "        Returns:\n",
    "            accuracy, f1 score, precision, recall : evaluation metrics generated from model predictions and ground truth labels\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        Y_pred = self.predict(X, prepare_input=prepare_input)\n",
    "        return calc_metrics(Y, Y_pred, print_metrics=True)\n",
    "    \n",
    "    def save_model(self):\n",
    "\n",
    "        \"\"\"Saves the model to the checkpoint location\"\"\"\n",
    "\n",
    "        self.model.save(self.model_path)\n",
    "\n",
    "        if self.augment_lda:\n",
    "            self.lda_model.save_lda()\n",
    "        print(\"Model saved.\")\n",
    "                    \n",
    "    def load_model(self):\n",
    "\n",
    "        \"\"\"If possible, loads the model from save files \n",
    "\n",
    "        Returns:\n",
    "            (bool) : whether the model could be loaded / save files exist\n",
    "\n",
    "        \"\"\"\n",
    "        if os.path.exists(self.tokenizer_path) and os.path.exists(self.model_path):\n",
    "            self.model = models.load_model(self.model_path)\n",
    "            self._load_tokenizer()\n",
    "        else: return False\n",
    "\n",
    "        if self.augment_lda:\n",
    "            self.lda_model = LDA_Model( ckpt_folder=self.lda_folder, save_suffix=self.save_suffix)\n",
    "            return self.lda_model.load_lda()\n",
    "\n",
    "        print(\"Model loaded.\")\n",
    "        return True\n",
    "    \n",
    "    def _train_tokenize(self, X, filters='\"#$%&@123456789' ):\n",
    "\n",
    "        \"\"\"Trains the tokenizer on input data and saves it to a save file.\n",
    "        \n",
    "        Args:\n",
    "            \n",
    "            X (np.ndarray) : input array of tweet strings\n",
    "            filters : the symbols that the tokenizer will eliminate while processing the data\n",
    "\n",
    "        Returns:\n",
    "            X_tokenized (np.ndarray) : input tweets converted to rows of token ids from the tokenizer\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        self.tokenizer = Tokenizer( filters=filters)\n",
    "        self.tokenizer.fit_on_texts(X)\n",
    "        \n",
    "        X_tokenized = self.tokenizer.texts_to_sequences(X)\n",
    "        X_tokenized = pad_sequences(X_tokenized, maxlen=self.max_len, padding='post')\n",
    "        \n",
    "        self._save_tokenizer()\n",
    "        \n",
    "        return X_tokenized\n",
    "            \n",
    "    def _tokenize(self, X):\n",
    "\n",
    "        \"\"\"Tokenizes input data.\n",
    "        \n",
    "        Args:\n",
    "            \n",
    "            X (np.ndarray) : input array of tweet strings\n",
    "        Returns:\n",
    "            X_tokenized (np.ndarray) : input tweets converted to rows of token ids from the tokenizer\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        X_tokenized = self.tokenizer.texts_to_sequences(X)\n",
    "        X_tokenized = pad_sequences(X_tokenized, maxlen=self.max_len, padding='post')\n",
    "        return X_tokenized\n",
    "        \n",
    "    \n",
    "    def _save_tokenizer(self):\n",
    "        \"\"\"Saves the tokenizer to save file\"\"\"\n",
    "    \n",
    "        tokenizer_json = self.tokenizer.to_json()\n",
    "        \n",
    "        with io.open(self.tokenizer_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "        print(\"Tokenizer saved to json file.\")\n",
    "    \n",
    "    def _load_tokenizer(self):\n",
    "        \"\"\"Loads the tokenizer from save file\"\"\"\n",
    "        \n",
    "        with open(self.tokenizer_path) as f:\n",
    "            tokenizer_json = json.load(f)\n",
    "            self.tokenizer = tokenizer_from_json(tokenizer_json)\n",
    "        print(\"Tokenizer loaded from save file.\")\n",
    "            \n",
    "    def _load_stats(self):\n",
    "        \"\"\"Loads statistics vectors corresponding to the frequency of potentially meaningful indicators in the original unprocessed training data\"\"\"\n",
    "         \n",
    "        path_pos_stat =  os.path.join(self.data_path, \"pos_stats.csv\")\n",
    "        path_neg_stat =  os.path.join(self.data_path, \"neg_stats.csv\")\n",
    "        \n",
    "        return load_stats(path_pos_stat, path_neg_stat)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _prepare_inputs(self, X, Y, random_state=0):\n",
    "\n",
    "        \"\"\"Prepares final model inputs depending on model configuration \n",
    "        \n",
    "        Args:\n",
    "            \n",
    "            X (np.ndarray) : array of tweet strings\n",
    "            Y (np.ndarray) : ground truth labels \n",
    "            random_state (int) : the seed used to condition random numbers for reproducible results\n",
    "\n",
    "        Returns:\n",
    "            train_inputs (list of np.ndarrays) : list of all model training input arrays\n",
    "            dev_inputs   (list of np.ndarrays) : list of all model training input arrays \n",
    "            X_train  (np.ndarray) : unprocessed train input to train the embedding model\n",
    "            Y_train  (np.ndarray) : train ground truth labels  \n",
    "            Y_dev    (np.ndarray) : dev ground truth labels \n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.augment_stats:    \n",
    "                                                                   \n",
    "            #loading statistics vectors  \n",
    "            try:                                                      \n",
    "                X_stats = self._load_stats()  \n",
    "                print(\"Stats vectors extracted.\")\n",
    "            except:\n",
    "                print(\"Stats vectors could not be loaded. Extracting stats.\")\n",
    "                X_stats = np.array( sentence_statistics(X) )\n",
    "                                                                   \n",
    "            #preparing model training and dev inputs\n",
    "            X_train, X_dev, Y_train, Y_dev, X_train_stats, X_dev_stats = train_test_split(X, Y, X_stats, test_size=0.1, random_state = random_state)\n",
    "            \n",
    "        else:\n",
    "            X_train, X_dev,Y_train, Y_dev = train_test_split(X, Y, test_size=0.1, random_state = random_state)\n",
    "        \n",
    "        # Tokenizing the dataset\n",
    "        X_train_tokenized = self._train_tokenize(X_train)\n",
    "        X_dev_tokenized = self._tokenize( X_dev)\n",
    "        \n",
    "        #creating input lists for keras model\n",
    "        train_inputs = [X_train_tokenized]\n",
    "        dev_inputs   = [X_dev_tokenized]\n",
    "\n",
    "        #adding extra inputs for model extensions\n",
    "        \n",
    "        if self.augment_stats:\n",
    "            train_inputs.append(X_train_stats)\n",
    "            dev_inputs.append(X_dev_stats)\n",
    "\n",
    "        if self.augment_lda:\n",
    "            self.lda_model = LDA_Model(num_topics = 10, ckpt_folder=self.lda_folder, save_suffix=self.save_suffix)\n",
    "            print(\"Training LDA Model\")\n",
    "            X_train_topics = self.lda_model.train(X_train)\n",
    "            X_dev_topics   = self.lda_model.process_new_data(X_dev)\n",
    "                                                                   \n",
    "            train_inputs.append(X_train_topics)\n",
    "            dev_inputs.append(X_dev_topics)\n",
    "            \n",
    "        return train_inputs, dev_inputs, X_train, Y_train, Y_dev\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682e06a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "\n",
      "embed: glove | standard params + attention + model statistics + lexical features + avg sent embedss  + 2 conv + 2 dense\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Stats vectors extracted.\n",
      "Tokenizer saved to json file.\n",
      "Training LDA Model\n",
      "LDA model successfuly loaded.\n",
      "Vocab Len: 373780\n",
      "Embedding Vector + Vader Len: 110\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 150, 110)     41115910    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 150, 200)     168800      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 150, 200)     0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (attention)         (None, 200)          350         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           3216        attention_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 26)           0           dense_1[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 36)           0           concatenate[0][0]                \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            37          concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 41,288,313\n",
      "Trainable params: 172,403\n",
      "Non-trainable params: 41,115,910\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n",
      "4500/4500 [==============================] - 1266s 279ms/step - loss: 0.4574 - accuracy: 0.7751 - val_loss: 0.3491 - val_accuracy: 0.8410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_4_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_4_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: checkpoints\\biLSTM\\biLSTM_glove_d100_vader_lda_stats_param_search_\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: checkpoints\\biLSTM\\biLSTM_glove_d100_vader_lda_stats_param_search_\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20\n",
      "4500/4500 [==============================] - 1287s 286ms/step - loss: 0.3345 - accuracy: 0.8484 - val_loss: 0.3202 - val_accuracy: 0.8560\n"
     ]
    }
   ],
   "source": [
    "line_str = \"----------------------------------------\\n\"\n",
    "star_str = \"****************************************\\n\"\n",
    "\n",
    "data_folder  = 'twitter-datasets/data/'\n",
    "glove_folder = 'twitter-datasets/glove_twitter'\n",
    "ckpt_folder  = \"checkpoints\"\n",
    "\n",
    "logging_file = os.path.join(ckpt_folder, \"param_search_logs\", \"param_search_logs.txt\") \n",
    "\n",
    "\n",
    "data_path     = data_folder + data_prefix + \"train_\"\n",
    "path_pos = data_path + \"pos_full.txt\"\n",
    "path_neg = data_path + \"neg_full.txt\"\n",
    "\n",
    "base_save_suffix = \"param_search_\"\n",
    "\n",
    "tweets, labels = load_tweets(path_pos, path_neg)\n",
    "\n",
    "for embed_type in embed_type_list:\n",
    "    with tf.device(device_name):\n",
    "\n",
    "                    model_config = f'embed: {embed_type} | standard params + attention + model statistics + lexical features + avg sent embedss  + 2 conv + 2 dense\\n'\n",
    "                    print(star_str)\n",
    "                    print(model_config)\n",
    "                    print(line_str)\n",
    "                    \n",
    "                    df=open(logging_file,'a')\n",
    "                    df.write(star_str )\n",
    "                    df.write(model_config )\n",
    "                    df.write(line_str )  \n",
    "                    df.close()\n",
    "                    \n",
    "              \n",
    "\n",
    "                    model = biLSTM_model1(save_suffix =base_save_suffix, \n",
    "                                         \n",
    "                                           embed_type = embed_type,\n",
    "\n",
    "                                           #model extension options\n",
    "                                           augment_stats   = True, \n",
    "                                           augment_vader   = True, \n",
    "                                           augment_lda     = True,\n",
    "\n",
    "                                           #path params\n",
    "                                           ckpt_folder = ckpt_folder,\n",
    "                                           embed_path = glove_folder, \n",
    "                                          )\n",
    "                    \n",
    "                    \n",
    "\n",
    "\n",
    "                    accuracy, f1, precision, recall = model.train_model( tweets, labels, num_conv1D = 0)\n",
    "                    \n",
    "                    model_result = f\"accuracy: {accuracy}, f1: {f1}, precision: {precision}, recall: {recall}\"\n",
    "                    \n",
    "                    df=open(logging_file,'a')\n",
    "                    df.write(model_result )\n",
    "                    df.close()\n",
    "                    \n",
    "                    del model\n",
    "                    K.clear_session()\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77720d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
